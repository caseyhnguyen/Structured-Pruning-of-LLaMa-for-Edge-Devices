{
  "best_metric": 1.6623727083206177,
  "best_model_checkpoint": "tune_log/llama3_tune/checkpoint-600",
  "epoch": 1.5434083601286175,
  "eval_steps": 100,
  "global_step": 600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.002572347266881029,
      "grad_norm": 49.46699142456055,
      "learning_rate": 1.0000000000000001e-07,
      "loss": 2.8638,
      "step": 1
    },
    {
      "epoch": 0.02572347266881029,
      "grad_norm": 49.08950424194336,
      "learning_rate": 1.0000000000000002e-06,
      "loss": 2.922,
      "step": 10
    },
    {
      "epoch": 0.05144694533762058,
      "grad_norm": 48.37998962402344,
      "learning_rate": 1.9000000000000002e-06,
      "loss": 2.9511,
      "step": 20
    },
    {
      "epoch": 0.07717041800643087,
      "grad_norm": 51.768611907958984,
      "learning_rate": 2.8000000000000003e-06,
      "loss": 2.9043,
      "step": 30
    },
    {
      "epoch": 0.10289389067524116,
      "grad_norm": 53.30964660644531,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 2.8426,
      "step": 40
    },
    {
      "epoch": 0.12861736334405144,
      "grad_norm": 52.24892044067383,
      "learning_rate": 4.800000000000001e-06,
      "loss": 2.7661,
      "step": 50
    },
    {
      "epoch": 0.15434083601286175,
      "grad_norm": 31.41520118713379,
      "learning_rate": 5.8e-06,
      "loss": 2.621,
      "step": 60
    },
    {
      "epoch": 0.18006430868167203,
      "grad_norm": 28.683671951293945,
      "learning_rate": 6.800000000000001e-06,
      "loss": 2.592,
      "step": 70
    },
    {
      "epoch": 0.2057877813504823,
      "grad_norm": 18.933910369873047,
      "learning_rate": 7.800000000000002e-06,
      "loss": 2.471,
      "step": 80
    },
    {
      "epoch": 0.2315112540192926,
      "grad_norm": 17.622539520263672,
      "learning_rate": 8.8e-06,
      "loss": 2.3685,
      "step": 90
    },
    {
      "epoch": 0.2572347266881029,
      "grad_norm": 15.546825408935547,
      "learning_rate": 9.800000000000001e-06,
      "loss": 2.2777,
      "step": 100
    },
    {
      "epoch": 0.2572347266881029,
      "eval_yahma/alpaca-cleaned_loss": 2.234194278717041,
      "eval_yahma/alpaca-cleaned_runtime": 40.6842,
      "eval_yahma/alpaca-cleaned_samples_per_second": 49.159,
      "eval_yahma/alpaca-cleaned_steps_per_second": 6.145,
      "step": 100
    },
    {
      "epoch": 0.2829581993569132,
      "grad_norm": 14.42218017578125,
      "learning_rate": 9.924812030075189e-06,
      "loss": 2.1947,
      "step": 110
    },
    {
      "epoch": 0.3086816720257235,
      "grad_norm": 14.036008834838867,
      "learning_rate": 9.830827067669174e-06,
      "loss": 2.0989,
      "step": 120
    },
    {
      "epoch": 0.33440514469453375,
      "grad_norm": 11.632598876953125,
      "learning_rate": 9.736842105263159e-06,
      "loss": 2.016,
      "step": 130
    },
    {
      "epoch": 0.36012861736334406,
      "grad_norm": 16.444250106811523,
      "learning_rate": 9.642857142857144e-06,
      "loss": 1.9963,
      "step": 140
    },
    {
      "epoch": 0.3858520900321543,
      "grad_norm": 11.27433967590332,
      "learning_rate": 9.54887218045113e-06,
      "loss": 1.9564,
      "step": 150
    },
    {
      "epoch": 0.4115755627009646,
      "grad_norm": 12.517718315124512,
      "learning_rate": 9.454887218045113e-06,
      "loss": 1.9349,
      "step": 160
    },
    {
      "epoch": 0.43729903536977494,
      "grad_norm": 12.449015617370605,
      "learning_rate": 9.360902255639098e-06,
      "loss": 1.8998,
      "step": 170
    },
    {
      "epoch": 0.4630225080385852,
      "grad_norm": 14.846171379089355,
      "learning_rate": 9.266917293233083e-06,
      "loss": 1.8526,
      "step": 180
    },
    {
      "epoch": 0.4887459807073955,
      "grad_norm": 10.876518249511719,
      "learning_rate": 9.172932330827068e-06,
      "loss": 1.8542,
      "step": 190
    },
    {
      "epoch": 0.5144694533762058,
      "grad_norm": 11.372624397277832,
      "learning_rate": 9.078947368421054e-06,
      "loss": 1.8296,
      "step": 200
    },
    {
      "epoch": 0.5144694533762058,
      "eval_yahma/alpaca-cleaned_loss": 1.845731258392334,
      "eval_yahma/alpaca-cleaned_runtime": 40.6527,
      "eval_yahma/alpaca-cleaned_samples_per_second": 49.197,
      "eval_yahma/alpaca-cleaned_steps_per_second": 6.15,
      "step": 200
    },
    {
      "epoch": 0.5401929260450161,
      "grad_norm": 12.05859088897705,
      "learning_rate": 8.984962406015039e-06,
      "loss": 1.7996,
      "step": 210
    },
    {
      "epoch": 0.5659163987138264,
      "grad_norm": 12.659111022949219,
      "learning_rate": 8.890977443609024e-06,
      "loss": 1.7693,
      "step": 220
    },
    {
      "epoch": 0.5916398713826366,
      "grad_norm": 13.797011375427246,
      "learning_rate": 8.796992481203007e-06,
      "loss": 1.8094,
      "step": 230
    },
    {
      "epoch": 0.617363344051447,
      "grad_norm": 11.763595581054688,
      "learning_rate": 8.703007518796993e-06,
      "loss": 1.7698,
      "step": 240
    },
    {
      "epoch": 0.6430868167202572,
      "grad_norm": 11.742948532104492,
      "learning_rate": 8.609022556390978e-06,
      "loss": 1.7575,
      "step": 250
    },
    {
      "epoch": 0.6688102893890675,
      "grad_norm": 12.627413749694824,
      "learning_rate": 8.515037593984963e-06,
      "loss": 1.771,
      "step": 260
    },
    {
      "epoch": 0.6945337620578779,
      "grad_norm": 13.873994827270508,
      "learning_rate": 8.421052631578948e-06,
      "loss": 1.735,
      "step": 270
    },
    {
      "epoch": 0.7202572347266881,
      "grad_norm": 13.339964866638184,
      "learning_rate": 8.327067669172933e-06,
      "loss": 1.7315,
      "step": 280
    },
    {
      "epoch": 0.7459807073954984,
      "grad_norm": 14.018712043762207,
      "learning_rate": 8.233082706766919e-06,
      "loss": 1.7372,
      "step": 290
    },
    {
      "epoch": 0.7717041800643086,
      "grad_norm": 13.382678031921387,
      "learning_rate": 8.139097744360904e-06,
      "loss": 1.7304,
      "step": 300
    },
    {
      "epoch": 0.7717041800643086,
      "eval_yahma/alpaca-cleaned_loss": 1.7534540891647339,
      "eval_yahma/alpaca-cleaned_runtime": 40.6462,
      "eval_yahma/alpaca-cleaned_samples_per_second": 49.205,
      "eval_yahma/alpaca-cleaned_steps_per_second": 6.151,
      "step": 300
    },
    {
      "epoch": 0.797427652733119,
      "grad_norm": 13.216289520263672,
      "learning_rate": 8.045112781954887e-06,
      "loss": 1.7302,
      "step": 310
    },
    {
      "epoch": 0.8231511254019293,
      "grad_norm": 15.026366233825684,
      "learning_rate": 7.951127819548872e-06,
      "loss": 1.7151,
      "step": 320
    },
    {
      "epoch": 0.8488745980707395,
      "grad_norm": 12.041139602661133,
      "learning_rate": 7.857142857142858e-06,
      "loss": 1.7042,
      "step": 330
    },
    {
      "epoch": 0.8745980707395499,
      "grad_norm": 14.398072242736816,
      "learning_rate": 7.763157894736843e-06,
      "loss": 1.7108,
      "step": 340
    },
    {
      "epoch": 0.9003215434083601,
      "grad_norm": 12.449389457702637,
      "learning_rate": 7.669172932330828e-06,
      "loss": 1.6903,
      "step": 350
    },
    {
      "epoch": 0.9260450160771704,
      "grad_norm": 14.071640968322754,
      "learning_rate": 7.575187969924813e-06,
      "loss": 1.7193,
      "step": 360
    },
    {
      "epoch": 0.9517684887459807,
      "grad_norm": 14.355270385742188,
      "learning_rate": 7.481203007518798e-06,
      "loss": 1.6956,
      "step": 370
    },
    {
      "epoch": 0.977491961414791,
      "grad_norm": 12.511055946350098,
      "learning_rate": 7.387218045112783e-06,
      "loss": 1.6995,
      "step": 380
    },
    {
      "epoch": 1.0032154340836013,
      "grad_norm": 15.443365097045898,
      "learning_rate": 7.293233082706768e-06,
      "loss": 1.6846,
      "step": 390
    },
    {
      "epoch": 1.0289389067524115,
      "grad_norm": 15.182022094726562,
      "learning_rate": 7.199248120300752e-06,
      "loss": 1.7205,
      "step": 400
    },
    {
      "epoch": 1.0289389067524115,
      "eval_yahma/alpaca-cleaned_loss": 1.709005355834961,
      "eval_yahma/alpaca-cleaned_runtime": 40.6738,
      "eval_yahma/alpaca-cleaned_samples_per_second": 49.172,
      "eval_yahma/alpaca-cleaned_steps_per_second": 6.146,
      "step": 400
    },
    {
      "epoch": 1.0546623794212218,
      "grad_norm": 13.082403182983398,
      "learning_rate": 7.1052631578947375e-06,
      "loss": 1.6701,
      "step": 410
    },
    {
      "epoch": 1.0803858520900322,
      "grad_norm": 12.521461486816406,
      "learning_rate": 7.011278195488722e-06,
      "loss": 1.6916,
      "step": 420
    },
    {
      "epoch": 1.1061093247588425,
      "grad_norm": 15.319595336914062,
      "learning_rate": 6.917293233082707e-06,
      "loss": 1.6534,
      "step": 430
    },
    {
      "epoch": 1.1318327974276527,
      "grad_norm": 14.429985046386719,
      "learning_rate": 6.823308270676692e-06,
      "loss": 1.6604,
      "step": 440
    },
    {
      "epoch": 1.157556270096463,
      "grad_norm": 16.653846740722656,
      "learning_rate": 6.729323308270677e-06,
      "loss": 1.6471,
      "step": 450
    },
    {
      "epoch": 1.1832797427652733,
      "grad_norm": 13.839739799499512,
      "learning_rate": 6.6353383458646626e-06,
      "loss": 1.6653,
      "step": 460
    },
    {
      "epoch": 1.2090032154340835,
      "grad_norm": 14.994077682495117,
      "learning_rate": 6.541353383458648e-06,
      "loss": 1.6664,
      "step": 470
    },
    {
      "epoch": 1.234726688102894,
      "grad_norm": 14.766678810119629,
      "learning_rate": 6.447368421052632e-06,
      "loss": 1.6499,
      "step": 480
    },
    {
      "epoch": 1.2604501607717042,
      "grad_norm": 14.259476661682129,
      "learning_rate": 6.353383458646617e-06,
      "loss": 1.6515,
      "step": 490
    },
    {
      "epoch": 1.2861736334405145,
      "grad_norm": 13.773926734924316,
      "learning_rate": 6.259398496240602e-06,
      "loss": 1.6674,
      "step": 500
    },
    {
      "epoch": 1.2861736334405145,
      "eval_yahma/alpaca-cleaned_loss": 1.681194543838501,
      "eval_yahma/alpaca-cleaned_runtime": 40.6812,
      "eval_yahma/alpaca-cleaned_samples_per_second": 49.163,
      "eval_yahma/alpaca-cleaned_steps_per_second": 6.145,
      "step": 500
    },
    {
      "epoch": 1.3118971061093248,
      "grad_norm": 14.386910438537598,
      "learning_rate": 6.165413533834587e-06,
      "loss": 1.6345,
      "step": 510
    },
    {
      "epoch": 1.337620578778135,
      "grad_norm": 19.003751754760742,
      "learning_rate": 6.071428571428571e-06,
      "loss": 1.6479,
      "step": 520
    },
    {
      "epoch": 1.3633440514469453,
      "grad_norm": 14.285270690917969,
      "learning_rate": 5.977443609022557e-06,
      "loss": 1.6333,
      "step": 530
    },
    {
      "epoch": 1.3890675241157555,
      "grad_norm": 14.835387229919434,
      "learning_rate": 5.883458646616542e-06,
      "loss": 1.6601,
      "step": 540
    },
    {
      "epoch": 1.414790996784566,
      "grad_norm": 14.01784610748291,
      "learning_rate": 5.789473684210527e-06,
      "loss": 1.6453,
      "step": 550
    },
    {
      "epoch": 1.4405144694533762,
      "grad_norm": 14.34008502960205,
      "learning_rate": 5.695488721804512e-06,
      "loss": 1.6526,
      "step": 560
    },
    {
      "epoch": 1.4662379421221865,
      "grad_norm": 14.366273880004883,
      "learning_rate": 5.601503759398497e-06,
      "loss": 1.6321,
      "step": 570
    },
    {
      "epoch": 1.4919614147909968,
      "grad_norm": 13.808174133300781,
      "learning_rate": 5.507518796992481e-06,
      "loss": 1.6109,
      "step": 580
    },
    {
      "epoch": 1.517684887459807,
      "grad_norm": 15.071181297302246,
      "learning_rate": 5.413533834586467e-06,
      "loss": 1.6458,
      "step": 590
    },
    {
      "epoch": 1.5434083601286175,
      "grad_norm": 15.613896369934082,
      "learning_rate": 5.319548872180451e-06,
      "loss": 1.6308,
      "step": 600
    },
    {
      "epoch": 1.5434083601286175,
      "eval_yahma/alpaca-cleaned_loss": 1.6623727083206177,
      "eval_yahma/alpaca-cleaned_runtime": 40.8146,
      "eval_yahma/alpaca-cleaned_samples_per_second": 49.002,
      "eval_yahma/alpaca-cleaned_steps_per_second": 6.125,
      "step": 600
    }
  ],
  "logging_steps": 10,
  "max_steps": 1164,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 200,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 5.17349516967936e+17,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
