{
  "os": "Linux-6.5.0-28-generic-x86_64-with-glibc2.35",
  "python": "3.10.12",
  "startedAt": "2024-12-05T05:59:06.376695Z",
  "args": [
    "--prune_model",
    "prune_log/llama3_prune/pytorch_model.bin",
    "--data_path",
    "yahma/alpaca-cleaned",
    "--lora_r",
    "16",
    "--num_epochs",
    "3",
    "--learning_rate",
    "1e-5",
    "--batch_size",
    "128",
    "--output_dir",
    "tune_log/llama3_tune",
    "--wandb_project",
    "llama_tune"
  ],
  "program": "/workspace/LLM-Pruner/post_training.py",
  "codePath": "post_training.py",
  "git": {
    "remote": "https://github.com/horseee/LLM-Pruner.git",
    "commit": "128a07d977f9b205d60ab14cfbc6a78f8a8e39d2"
  },
  "email": "aa192002@gmail.com",
  "root": "/workspace/LLM-Pruner",
  "host": "2e4fb9554891",
  "username": "root",
  "executable": "/workspace/LLM-Pruner/venv/bin/python",
  "codePathLocal": "post_training.py",
  "cpu_count": 128,
  "cpu_count_logical": 255,
  "gpu": "NVIDIA A100-SXM4-80GB",
  "gpu_count": 2,
  "disk": {
    "/": {
      "total": "85899345920",
      "used": "42922950656"
    }
  },
  "memory": {
    "total": "1081976475648"
  },
  "cpu": {
    "count": 128,
    "countLogical": 255
  },
  "gpu_nvidia": [
    {
      "name": "NVIDIA A100-SXM4-80GB",
      "memoryTotal": "85899345920",
      "cudaCores": 6912,
      "architecture": "Ampere"
    },
    {
      "name": "NVIDIA A100-SXM4-80GB",
      "memoryTotal": "85899345920",
      "cudaCores": 6912,
      "architecture": "Ampere"
    }
  ],
  "cudaVersion": "12.7"
}